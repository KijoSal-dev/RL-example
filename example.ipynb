{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMIYsS3b6thu2ueZYXZ34h0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KijoSal-dev/RL-example/blob/main/example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6-2fIBdf9Uu",
        "outputId": "e20f60de-a940-40b1-c272-5462472aebe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trained Q-table:\n",
            "[[0.73509189 0.77378094 0.69819565 0.73509189]\n",
            " [0.7350869  0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.77378094 0.81450625 0.         0.73509189]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.81450617 0.         0.857375   0.77378093]\n",
            " [0.81450625 0.9025     0.81450625 0.        ]\n",
            " [0.857375   0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.        ]\n",
            " [0.         0.9025     0.95       0.857375  ]\n",
            " [0.90249954 0.94998784 1.         0.81450625]\n",
            " [0.         0.         0.         0.        ]]\n",
            "Total reward earned during test: 1.0\n"
          ]
        }
      ],
      "source": [
        "# Import the gym library for the environment and numpy for numerical operations.\n",
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Create the FrozenLake environment.\n",
        "# 'is_slippery=False' makes the environment deterministic (simpler for demonstration).\n",
        "# Using new_step_api=True for compatibility with newer gym versions.\n",
        "env = gym.make(\"FrozenLake-v1\", is_slippery=False, new_step_api=True)\n",
        "\n",
        "# Initialize the Q-table with zeros.\n",
        "# Rows correspond to states and columns to possible actions.\n",
        "Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n",
        "\n",
        "# Set hyperparameters for the Q-learning algorithm.\n",
        "n_episodes = 2000       # Total episodes to run the training.\n",
        "max_steps = 100         # Maximum steps per episode.\n",
        "learning_rate = 0.8     # Learning rate to control update magnitude.\n",
        "discount_factor = 0.95  # Discount factor for future rewards.\n",
        "epsilon = 1.0           # Exploration rate (starting value).\n",
        "max_epsilon = 1.0       # Maximum exploration probability.\n",
        "min_epsilon = 0.01      # Minimum exploration probability.\n",
        "decay_rate = 0.005      # Exponential decay rate for epsilon.\n",
        "\n",
        "# Q-learning algorithm:\n",
        "for episode in range(n_episodes):\n",
        "    # Reset environment for a new episode with new_step_api\n",
        "    # Assign the result of env.reset() to a variable and then access state and info.\n",
        "    reset_result = env.reset()\n",
        "    state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
        "    info = reset_result[1] if isinstance(reset_result, tuple) and len(reset_result) > 1 else {}\n",
        "\n",
        "    done = False         # Indicator whether the episode is finished.\n",
        "\n",
        "    for step in range(max_steps):\n",
        "        # Generate a random number to decide whether to explore or exploit.\n",
        "        exp_exp_tradeoff = np.random.uniform(0, 1)\n",
        "\n",
        "        # Epsilon-greedy strategy:\n",
        "        # If the random number is greater than epsilon, we exploit (choose best action).\n",
        "        # Otherwise, we explore by selecting a random action.\n",
        "        if exp_exp_tradeoff > epsilon:\n",
        "            action = np.argmax(Q_table[state, :])  # Best known action.\n",
        "        else:\n",
        "            action = env.action_space.sample()      # Random action.\n",
        "\n",
        "        # Take the chosen action, and observe the next state, reward, and if the episode is done.\n",
        "        new_state, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Update the Q-table using the Q-learning update rule:\n",
        "        # Q(s, a) = Q(s, a) + learning_rate * [reward + discount * max(Q(new_state)) - Q(s, a)]\n",
        "        Q_table[state, action] = Q_table[state, action] + learning_rate * (\n",
        "            reward + discount_factor * np.max(Q_table[new_state, :]) - Q_table[state, action]\n",
        "        )\n",
        "\n",
        "        # Transition to the new state.\n",
        "        state = new_state\n",
        "\n",
        "        # If the episode is finished (reached a terminal state), exit the loop.\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    # Decay epsilon after each episode.\n",
        "    # This gradually reduces exploration as the agent learns more about the environment.\n",
        "    epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
        "\n",
        "# After training, print the final Q-table.\n",
        "print(\"Trained Q-table:\")\n",
        "print(Q_table)\n",
        "\n",
        "# --- Testing the trained agent ---\n",
        "# Reset the environment for testing with new_step_api\n",
        "reset_result = env.reset()\n",
        "state = reset_result[0] if isinstance(reset_result, tuple) else reset_result\n",
        "info = reset_result[1] if isinstance(reset_result, tuple) and len(reset_result) > 1 else {}\n",
        "total_reward = 0     # Initialize reward accumulator.\n",
        "for step in range(max_steps):\n",
        "    # Always select the best action based on the current Q-table (exploitation).\n",
        "    action = np.argmax(Q_table[state, :])\n",
        "    new_state, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    total_reward += reward  # Accumulate the reward.\n",
        "    state = new_state       # Move to the next state.\n",
        "    if done:\n",
        "        break\n",
        "\n",
        "print(\"Total reward earned during test:\", total_reward)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5b4d51c",
        "outputId": "084dfba9-d57d-4761-8c04-ab2f0b7a5b39"
      },
      "source": [
        "%pip install numpy==1.24.0"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.0 in /usr/local/lib/python3.11/dist-packages (1.24.0)\n"
          ]
        }
      ]
    }
  ]
}